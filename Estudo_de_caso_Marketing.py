# -*- coding: utf-8 -*-
"""LLMs para Marketing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pR-RaayvQ4WGhUDZ9Ll-OJin0dcNM2Wi

# Projeto para Departamento de Marketing

#### Instala√ß√£o das bibliotecas
"""

!pip install -q langchain langchain-community langchain-groq ipywidgets

"""#### Importar bibliotecas"""

from google.colab import widgets
import ipywidgets as widgets

from langchain_groq import ChatGroq
import os
import getpass

"""## Cria√ß√£o dos campos - Interface
Campos apresentados no slide do estudo de caso


*   Plataforma de destino (ex: Blog, Instagram, LinkedIn, E-mail)
*   Tom da mensagem (ex: Informativo, Inspirador, Urgente, Informal)
*   Comprimento do texto (ex: Curto, M√©dio, Longo)
*   Tema ou t√≥pico (ex: Alimenta√ß√£o, sa√∫de mental, exames de rotina, cuidados, etc.)
*   P√∫blico-alvo (Jovens adultos, Fam√≠lias, Idosos, Geral, etc.)
*   Op√ß√µes adicionais:
    *   Incluir chamada para a√ß√£o (ex: "Agendar consulta" ou "Converse com um especialista")
    *   Retornar hashtags
    *   Inserir palavras-chaves para incluir no meio do texto



"""

topic = widgets.Text(
    description = 'Tema:',
    placeholder = 'Ex: sa√∫de mental, alimenta√ß√£o saud√°vel, preven√ß√£o, etc.'
)

"""#### Exibindo o widget"""

display(topic)

topic.value

"""#### Ajustando propriedades do campo"""

topic = widgets.Text(
    description = 'Tema:',
    placeholder = 'Ex: sa√∫de mental, alimenta√ß√£o saud√°vel, preven√ß√£o, etc.',
    layout = widgets.Layout(width='500px')
)
display(topic)

"""#### Outros formatos de campos"""

w_dropdown = '250px'

platform = widgets.Dropdown(
    options = ['Blog', 'E-mail', 'LinkedIn', 'Instagram', 'Facebook'],
    description = 'Plataforma',
    layout = widgets.Layout(width = w_dropdown)
)
display(platform)

w_dropdown = '250px'

platform = widgets.Dropdown(
    options = ['Blog', 'E-mail', 'LinkedIn', 'Instagram', 'Facebook'],
    description = 'Plataforma:',
    layout = widgets.Layout(width = w_dropdown)
)
tone = widgets.Dropdown(
    options = ['Normal', 'Informativo', 'Inspirador', 'Urgente', 'Informal'],
    description = 'Tom:',
    layout = widgets.Layout(width = w_dropdown)
)
length = widgets.Dropdown(
    options = ['Curto', 'M√©dio', 'Longo'],
    description = 'Tamanho:',
    layout = widgets.Layout(width = w_dropdown)
)
audience = widgets.Dropdown(
    options = ['Geral', 'Jovens adultos', 'Fam√≠lias', 'Idosos', 'Adolescentes'],
    description = 'P√∫blico-alvo:',
    layout = widgets.Layout(width = w_dropdown)
)
display(platform, tone, length, audience)

cta = widgets.Checkbox(
    value = False,
    description = 'Incluir CTA'
)

hashtags = widgets.Checkbox(
    value = False,
    description = 'Retornas Hashtags'
)

display(cta)

cta.value

keywords = widgets.Textarea(
    description = 'Palavras-chave (SEO)',
    placeholder = 'Ex: bem-estar, medicina preventiva...',
    layout = widgets.Layout(width = '500px', height = '50px')
)
display(keywords)

"""### Criando o bot√£o de gera√ß√£o"""

generate_button = widgets.Button(
    description = 'Gerar conte√∫do',
)
display(generate_button)

"""### Exibi√ß√£o do resultado"""

output = widgets.Output(

)

"""### Definindo a√ß√£o do bot√£o"""

def generate_result(b):
    with output:
        output.clear_output()
        print("Ok!")

generate_button.on_click(generate_result)

display(generate_button, output)

"""### Exibindo os campos juntos na interface de sa√≠da"""

def create_form():
    return widgets.VBox(
        [
            topic,
            platform,
            tone,
            length,
            audience,
            cta,
            hashtags,
            keywords,
            generate_button,
            output
        ]
    )

form = create_form()
display(form)

"""# Conectando com a LLM"""

os.environ["GROQ_API_KEY"] =getpass.getpass()

id_model = "llama3-70b-8192" #@param{type: "string"}

llm = ChatGroq(
    model = id_model,
    temperature = 0.7,
    max_tokens = None,
    timeout = None,
    max_retries = 2,
)

"""## Formato das mensagens"""

prompt = "Ol√°! Quem √© voc√™?" # @param{type: "string"}

template = [
    ("system", "Voc√™ √© um redator profissional."),
    ("human", prompt)
]

res = llm.invoke(template)
res

res.content

from langchain_core.prompts import ChatPromptTemplate

prompt = "Ol√°! Quem √© voc√™?" #@param{type: "string"}

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um redator profissional."),
    ("human", "{prompt}")
])

chain = template | llm

res = chain.invoke({"prompt": prompt})
res.content

"""### Estendendo a chain para Output parser"""

from langchain_core.output_parsers import StrOutputParser

prompt = "Ol√°! Quem √© voc√™?" #@param{type: "string"}

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um redator profissional."),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
res

"""## Melhorando a exibi√ß√£o do resultado"""

def show_res(res):
    from IPython.display import Markdown
    display(Markdown(res))

show_res(res)

"""### Juntando em uma fun√ß√£o"""

def llm_generate(llm, prompt):
    template = ChatPromptTemplate.from_messages([
        ("system", "Voc√™ √© um redator profissional."),
        ("human", "{prompt}"),
    ])

    chain = template | llm | StrOutputParser()

    res = chain.invoke({"prompt": prompt})
    show_res(res)

prompt = "Escreva 5 dicas de sa√∫de" #@param{type: "string"}

llm_generate(llm, prompt)

prompt = "Escreva 5 dicas de sa√∫de mental" #@param{type: "string"}

llm_generate(llm, prompt)

prompt = "Escreva 5 dicas para quem quer ser arquiteto" #@param{type: "string"}

llm_generate(llm, prompt)

prompt = "Escreva quais s√£o as 5 ferramentas principais de um arquiteto" #@param{type: "string"}

llm_generate(llm, prompt)

prompt = "como reconhecer e diferenciar depressao de¬†bipolaridade" #@param{type: "string"}

llm_generate(llm, prompt)

"""# Construindo o promp de aplica√ß√£o

#### Estrutura de um prompt

Existem v√°rias t√©cnicas de engenharia de prompt, onde muitdas delas se baseiam em princ√≠pions parecidos. Uma abordagem simples para construiz um prompt mais completo √© adicionar a ele alguns 'Blocos' (componentes), que no caso seriam:
*   Papel (Role) - "quem" ele deve interpretar (mais sobre isso abaixo)
*   Tarefa (Task) - tarefa que deve realizar
*   Entrada (Input) - informa√ß√£o que pode ser usada como contexto para gerar uma resposta (por exemplo o faturamento mensal de uma empresa, ou um dado espec√≠fico sobre algo ou algu√©m)
*   Sa√≠da (Output) - qualquer que seja o resultado. Podemos especificar tamb√©m regras, como tamanho do resultado (medido em quantidade de palavras ou par√°grafos por exemplo)
*   Restri√ß√µes (Constraints) - o que queremos evitar na resposta. Por exemplo: "evite jarg√µes ou linguagem muito t√©cnica". "N√£o inclua sua an√°lise ou opini√£o".

### Interpreta√ß√£o de pap√©is - Role Prompting
"""

prompt = "Fale sobre chocolate em 1 par√°grafo"

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um historiador profissional."),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
show_res(res)

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um especialista em marketing digital."),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
show_res(res)

template = ChatPromptTemplate.from_messages([
    ("system", "Voc√™ √© um especialista em marketing digital com foco em SEO e escrita persuasiva."),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
show_res(res)

"""### Usando exemplos - One-Shot e Few-Shot Prompting
*   Zero-Shot -- O modelo responde sem exemplos, confiando apenas no treinamento.
*   One-Shot -- Um exemplo √© fornecido para orientar a resposta.
*   Few-Shot -- V√°rios exemplos ajudam o modelo a reconhecer padr√µes e melhorar a precis√£o.
"""

assunto = 'chocolate'

one_shot = f"""
Exemplo:
T√≠tulo: Voc√™ sabia que beber mais √°gua pode melhorar sua concentra√ß√£o?
Texto: A desidrata√ß√£o leve j√° √© suficiente para reduzir seu foco e energia no dia a dia. Mantenha uma garrafinha por perto e lembre-se de se hidratar ao longo do dia.
Hashtags: #hidrata√ß√£o #foconasaude

Agora gere um novo texto que fale sobre {assunto}
"""

#print(one_shot)

res = chain.invoke({"prompt": one_shot})
show_res(res)

"""O few-shot prompting, ou prompt com exemplos, demonstra √† IA a estrutura, estilo e abordagem desejados para a resposta, como a inclus√£o de hashtags ou a formula√ß√£o de t√≠tulos como perguntas, tornando o processo de instru√ß√£o mais intuitivo e eficiente do que apenas fornecer instru√ß√µes textuais. Embora exemplos possam ser combinados com texto para maior precis√£o, o few-shot prompting com m√∫ltiplos exemplos, como o que veremos a seguir, ajuda a IA a generalizar melhor o padr√£o esperado."""

few_shot = f"""
Exemplo 1:
T√≠tulo: Voc√™ sabia que beber mais √°gua pode melhorar sua concentra√ß√£o?
Texto: A desidrata√ß√£o leve j√° √© suficiente para reduzir seu foco e energia no dia a dia. Mantenha uma garrafinha por perto e lembre-se de se hidratar ao longo do dia.
Hashtags: #hidrata√ß√£o #foconasaude

Exemplo 2:
T√≠tulo: Comer carboidratos √† noite engorda: Mito ou verdade?
Texto: Esse √© um mito comum. O que realmente importa √© o total cal√≥rico do dia e a qualidade dos alimentos. Com orienta√ß√£o certa, d√° sim para comer bem √† noite sem culpa!
Hashtags: #nutricaosemmitos #equilibrioalimentar

Agora gere um novo texto que fale sobre {assunto}
"""

res = chain.invoke({"prompt": few_shot})
show_res(res)

"""#### Guiando o resultado com uma estrutura - Structured Prompting"""

form = create_form()
display(form)

prompt = f"""
Crie um post para {platform.value} com a seguinte estrutura:
1. Comece com uma pergunta provocativa.
2. Apresente um benef√≠cio claro relacionado ao tema.
3. Finalize com uma chamada para a√ß√£o (CTA) encorajando o leitor a buscar mais informa√ß√µes.

Tema: {topic.value}
P√∫blico-alvo: {audience.value}
Tom: {tone.value}
"""

print(prompt)

res = chain.invoke({"prompt": prompt})
show_res(res)

"""### Construindo o prompt final dinamicamente

Este prompt final ser√° constru√≠do a partir das vari√°veis do formul√°rio, organizado em itens leg√≠veis com - para f√°cil modifica√ß√£o e escalabilidade.

Cada linha fornecer√° instru√ß√µes claras (canal, tom, p√∫blico...), e op√ß√µes como hashtags ou CTAs ser√£o inclu√≠das condicionalmente usando express√µes inline em Python, adaptando o prompt √†s escolhas do usu√°rio.

Adicionaremos tamb√©m a instru√ß√£o para garantir que a sa√≠da seja limpa e pronta para uso.
"""

prompt = f"""
Escreva um texto com SEO otimizado sobre o tema '{topic.value}'.
Retorne em sua resposta apenas o texto final.
- Onde ser√° publicado: {platform.value}.
- Tom: {tone.value}.
- P√∫blico-alvo: {audience.value}.
- Comprimento: {length.value}.
- {"Inclua uma chamada para a√ß√£o clara." if cta.value else "N√£o inclua chamada para a√ß√£o"}
- {"Retorne ao final do texto hashtags relevantes." if hashtags.value else "N√£o inclua hashtags."}
{"- Palavras-chave que devem estar presentes nesse texto (para SEO): " + keywords.value if keywords.value else ""}
"""
print(prompt)

res = chain.invoke({"prompt": prompt})
show_res(res)

"""## Conclu√≠ndo a aplica√ß√£o"""

def llm_generate(llm, prompt):
  template = ChatPromptTemplate.from_messages([
      ("system", "Voc√™ √© um especialista em marketing digital com foco em SEO e escrita persuasiva."),
      ("human", "{prompt}"),
  ])

  chain = template | llm | StrOutputParser()

  res = chain.invoke({"prompt": prompt})
  return res

def generate_result(b):
  with output:
    output.clear_output()
    prompt = f"""
    Escreva um texto com SEO otimizado sobre o tema '{topic.value}'.
    Retorne em sua resposta apenas o texto final e n√£o inclua ela dentro de aspas.
    - Onde ser√° publicado: {platform.value}.
    - Tom: {tone.value}.
    - P√∫blico-alvo: {audience.value}.
    - Comprimento: {length.value}.
    - {"Inclua uma chamada para a√ß√£o clara." if cta.value else "N√£o inclua chamada para a√ß√£o"}
    - {"Retorne ao final do texto hashtags relevantes." if hashtags.value else "N√£o inclua hashtags."}
    {"- Palavras-chave que devem estar presentes nesse texto (para SEO): " + keywords.value if keywords.value else ""}
    """
    try:
      res = llm_generate(llm, prompt)
      show_res(res)
    except Exception as e:
      print(f"Erro: {e}")

output = widgets.Output()
generate_button = widgets.Button(description = "Gerar conte√∫do")
generate_button.on_click(generate_result)
form = create_form()

display(form)

"""## Escalando para outras √°reas e adicionando mais

---

campos

Para aumentar a flexibilidade na defini√ß√£o das op√ß√µes dos campos Dropdown, em vez de fix√°-las no c√≥digo, utilizaremos os formul√°rios do Colab com a anota√ß√£o `@param {type:"string"}`. Isso permite que o usu√°rio insira uma lista de valores separados por v√≠rgula diretamente em um campo ao lado da c√©lula de c√≥digo, que √© ent√£o convertida em uma lista Python e usada dinamicamente no par√¢metro options do widget.

Dessa forma, o formul√°rio se torna totalmente configur√°vel, permitindo f√°cil adi√ß√£o ou modifica√ß√£o das op√ß√µes dos dropdowns, como as do campo "comprimento", sem alterar o c√≥digo principal.

"""

opt_length = "Curto, M√©dio, Longo, 1 par√°grafo, 1 p√°gina" # @param {type:"string"}
print(opt_length)

options_length = [x.strip() for x in opt_length.split(",")]

length = widgets.Dropdown(
    options = options_length,
    description="Tamanho",
    layout=widgets.Layout(width=w_dropdown)
)

form = create_form()
output.clear_output()
display(form)

"""## Constru√ß√£o de interface com Streamlit

#### 1. Instala√ß√£o do Streamlit
"""

!pip install -q streamlit
!npm install -q localtunnel
!pip install -q python-dotenv

"""### 2. Cria√ß√£o do arquivo da aplica√ß√£o

Crie um arquivo chamado `app.py` (ou outro nome que preferir) com o conte√∫do do seu c√≥digo adaptado para Streamlit.

Antes de colocarmos o c√≥digo nesse arquivo, vamos criar o arquivo .env, para carregar as vari√°veis de ambiente. Aqui basta colocarmos a key do Groq, a mesma que usamos anteriormente. Deixe nesse formato: `GROQ_API_KEY=CHAVE_AQUI`

* Obs: o comando `%%writefile` no in√≠cio desse bloco de c√≥digo permite que a c√©lula do notebook seja salva como um arquivo externo, com o nome especificado. Ou seja, estamos criando um arquivo com esse nome e o conte√∫do ser√° tudo a partir da segunda linha do bloco abaixo

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile .env
# GROQ_API_KEY=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

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from langchain_groq import ChatGroq
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from dotenv import load_dotenv
# load_dotenv()
# 
# ## conex√£o com a LLM
# id_model = "llama3-70b-8192"
# llm = ChatGroq(
#     model=id_model,
#     temperature=0.7,
#     max_tokens=None,
#     timeout=None,
#     max_retries=2,
# )
# 
# ## fun√ß√£o de gera√ß√£o
# def llm_generate(llm, prompt):
#   template = ChatPromptTemplate.from_messages([
#       ("system", "Voc√™ √© um especialista em marketing digital com foco em SEO e escrita persuasiva."),
#       ("human", "{prompt}"),
#   ])
# 
#   chain = template | llm | StrOutputParser()
# 
#   res = chain.invoke({"prompt": prompt})
#   return res
# 
# st.set_page_config(page_title = "Gerador de conte√∫do ü§ñ", page_icon="ü§ñ")
# st.title("Gerador de conte√∫do")
# 
# # Campos do formul√°rio
# topic = st.text_input("Tema:", placeholder="Ex: sa√∫de mental, alimenta√ß√£o saud√°vel, preven√ß√£o, etc.")
# platform = st.selectbox("Plataforma:", ['Instagram', 'Facebook', 'LinkedIn', 'Blog', 'E-mail'])
# tone = st.selectbox("Tom:", ['Normal', 'Informativo', 'Inspirador', 'Urgente', 'Informal'])
# length = st.selectbox("Tamanho:", ['Curto', 'M√©dio', 'Longo'])
# audience = st.selectbox("P√∫blico-alvo:", ['Geral', 'Jovens adultos', 'Fam√≠lias', 'Idosos', 'Adolescentes'])
# cta = st.checkbox("Incluir CTA")
# hashtags = st.checkbox("Retornar Hashtags")
# keywords = st.text_area("Palavras-chave (SEO):", placeholder="Ex: bem-estar, medicina preventiva...")
# 
# if st.button("Gerar conte√∫do"):
#   prompt = f"""
#   Escreva um texto com SEO otimizado sobre o tema '{topic}'.
#   Retorne em sua resposta apenas o texto final e n√£o inclua ela dentro de aspas.
#   - Onde ser√° publicado: {platform}.
#   - Tom: {tone}.
#   - P√∫blico-alvo: {audience}.
#   - Comprimento: {length}.
#   - {"Inclua uma chamada para a√ß√£o clara." if cta else "N√£o inclua chamada para a√ß√£o"}
#   - {"Retorne ao final do texto hashtags relevantes." if hashtags else "N√£o inclua hashtags."}
#   {"- Palavras-chave que devem estar presentes nesse texto (para SEO): " + keywords if keywords else ""}
#   """
#   try:
#       res = llm_generate(llm, prompt)
#       st.markdown(res)
#   except Exception as e:
#       st.error(f"Erro: {e}")

"""### 3. Execu√ß√£o do Streamlit

Tendo nosso script pronto, basta executar o comando abaixo para rodar a nossa aplica√ß√£o pelo streamlit.
Isso far√° com que a aplica√ß√£o do Streamlit seja executada em segundo plano.
"""

!streamlit run app.py &>/content/logs.txt &

!wget -q -O - ipv4.icanhazip.com
!npx localtunnel --port 8501

"""---

## Rodando a LLM localmente

Se for um modelo open source n√≥s podemos fazer o download e rodar localmente em um provedor cloud (como nesse caso o colab) ou em nosso pr√≥prio computador.

### -> Para executar no Colab

**Importante:** Antes de realizar os pr√≥ximos passos, mude o ambiente de execu√ß√£o no Colab para usar GPU, que ser√° necess√°rio j√° que todo o processamento ser√° feito direto localmente no ambiente de execu√ß√£o do Colab. Para isso, selecione 'Ambiente de execu√ß√£o > Alterar o tipo de ambiente de execu√ß√£o' e na op√ß√£o 'Acelerador de hardware' selecione 'GPU'.

Al√©m das bibliotecas do langchain que instalamos, vamos precisar tamb√©m da biblioteca `langchain-huggingface`, `transformers` e `bitsandbytes`
"""

!pip install -q langchain langchain-community langchain-huggingface transformers

!pip install bitsandbytes-cuda110 bitsandbytes

from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
import torch

"""**Quantiza√ß√£o**

A execu√ß√£o de LLMs pode ser desafiadora devido aos recursos limitados, especialmente na vers√£o gratuita do Google Colab. Para contornar essa limita√ß√£o, al√©m de escolher modelos com menos par√¢metros podemos usar t√©cnicas de quantiza√ß√£o, como o `BitsAndBytesConfig` da biblioteca `transformers`, que permitem carregar e executar modelos massivos de forma eficiente sem comprometer significativamente o desempenho.
* Essas t√©cnicas reduzem os custos de mem√≥ria e computa√ß√£o ao representar pesos e ativa√ß√µes com tipos de dados de menor precis√£o, como inteiros de 8 bits (int8) ou at√© 4 bits, tornando vi√°vel o uso de modelos grandes mesmo em hardware limitado.
* Alternativas ao BitsAndBytesConfig: AutoGPTQ, AutoAWQ, etc.
* Para quem prefere evitar configura√ß√µes complexas de otimiza√ß√£o e manter a m√°xima qualidade, considere o uso via API.
* Mais detalhes sobre quantiza√ß√£o: https://huggingface.co/blog/4bit-transformers-bitsandbytes
"""

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

"""**Download do modelo**

Agora faremos o download e a configura√ß√£o de um modelo do HuggingFace usando o m√©todo `AutoModelForCausalLM.from_pretrained`. Este processo pode levar alguns minutos, pois o modelo tem alguns GB - mas no geral o download no Colab deve ser relativamente r√°pido.

> Para ver todos os modelos dispon√≠veis no Hugging Face, acesse: https://huggingface.co/models?pipeline_tag=text-generation

Escolhemos o Phi 3 (microsoft/Phi-3-mini-4k-instruct), um modelo menor mas que demonstrou ser muito interessante e com √≥timo custo benef√≠cio
 - https://huggingface.co/microsoft/Phi-3-mini-4k-instruct


"""

