# -*- coding: utf-8 -*-
"""LLMs para Marketing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pR-RaayvQ4WGhUDZ9Ll-OJin0dcNM2Wi

# Projeto para Departamento de Marketing

#### Instalação das bibliotecas
"""

!pip install -q langchain langchain-community langchain-groq ipywidgets

"""#### Importar bibliotecas"""

from google.colab import widgets
import ipywidgets as widgets

from langchain_groq import ChatGroq
import os
import getpass

"""## Criação dos campos - Interface
Campos apresentados no slide do estudo de caso


*   Plataforma de destino (ex: Blog, Instagram, LinkedIn, E-mail)
*   Tom da mensagem (ex: Informativo, Inspirador, Urgente, Informal)
*   Comprimento do texto (ex: Curto, Médio, Longo)
*   Tema ou tópico (ex: Alimentação, saúde mental, exames de rotina, cuidados, etc.)
*   Público-alvo (Jovens adultos, Famílias, Idosos, Geral, etc.)
*   Opções adicionais:
    *   Incluir chamada para ação (ex: "Agendar consulta" ou "Converse com um especialista")
    *   Retornar hashtags
    *   Inserir palavras-chaves para incluir no meio do texto



"""

topic = widgets.Text(
    description = 'Tema:',
    placeholder = 'Ex: saúde mental, alimentação saudável, prevenção, etc.'
)

"""#### Exibindo o widget"""

display(topic)

topic.value

"""#### Ajustando propriedades do campo"""

topic = widgets.Text(
    description = 'Tema:',
    placeholder = 'Ex: saúde mental, alimentação saudável, prevenção, etc.',
    layout = widgets.Layout(width='500px')
)
display(topic)

"""#### Outros formatos de campos"""

w_dropdown = '250px'

platform = widgets.Dropdown(
    options = ['Blog', 'E-mail', 'LinkedIn', 'Instagram', 'Facebook'],
    description = 'Plataforma',
    layout = widgets.Layout(width = w_dropdown)
)
display(platform)

w_dropdown = '250px'

platform = widgets.Dropdown(
    options = ['Blog', 'E-mail', 'LinkedIn', 'Instagram', 'Facebook'],
    description = 'Plataforma:',
    layout = widgets.Layout(width = w_dropdown)
)
tone = widgets.Dropdown(
    options = ['Normal', 'Informativo', 'Inspirador', 'Urgente', 'Informal'],
    description = 'Tom:',
    layout = widgets.Layout(width = w_dropdown)
)
length = widgets.Dropdown(
    options = ['Curto', 'Médio', 'Longo'],
    description = 'Tamanho:',
    layout = widgets.Layout(width = w_dropdown)
)
audience = widgets.Dropdown(
    options = ['Geral', 'Jovens adultos', 'Famílias', 'Idosos', 'Adolescentes'],
    description = 'Público-alvo:',
    layout = widgets.Layout(width = w_dropdown)
)
display(platform, tone, length, audience)

cta = widgets.Checkbox(
    value = False,
    description = 'Incluir CTA'
)

hashtags = widgets.Checkbox(
    value = False,
    description = 'Retornas Hashtags'
)

display(cta)

cta.value

keywords = widgets.Textarea(
    description = 'Palavras-chave (SEO)',
    placeholder = 'Ex: bem-estar, medicina preventiva...',
    layout = widgets.Layout(width = '500px', height = '50px')
)
display(keywords)

"""### Criando o botão de geração"""

generate_button = widgets.Button(
    description = 'Gerar conteúdo',
)
display(generate_button)

"""### Exibição do resultado"""

output = widgets.Output(

)

"""### Definindo ação do botão"""

def generate_result(b):
    with output:
        output.clear_output()
        print("Ok!")

generate_button.on_click(generate_result)

display(generate_button, output)

"""### Exibindo os campos juntos na interface de saída"""

def create_form():
    return widgets.VBox(
        [
            topic,
            platform,
            tone,
            length,
            audience,
            cta,
            hashtags,
            keywords,
            generate_button,
            output
        ]
    )

form = create_form()
display(form)

"""# Conectando com a LLM"""

os.environ["GROQ_API_KEY"] =getpass.getpass()

id_model = "llama3-70b-8192" #@param{type: "string"}

llm = ChatGroq(
    model = id_model,
    temperature = 0.7,
    max_tokens = None,
    timeout = None,
    max_retries = 2,
)

"""## Formato das mensagens"""

prompt = "Olá! Quem é você?" # @param{type: "string"}

template = [
    ("system", "Você é um redator profissional."),
    ("human", prompt)
]

res = llm.invoke(template)
res

res.content

from langchain_core.prompts import ChatPromptTemplate

prompt = "Olá! Quem é você?" #@param{type: "string"}

template = ChatPromptTemplate.from_messages([
    ("system", "Você é um redator profissional."),
    ("human", "{prompt}")
])

chain = template | llm

res = chain.invoke({"prompt": prompt})
res.content

"""### Estendendo a chain para Output parser"""

from langchain_core.output_parsers import StrOutputParser

prompt = "Olá! Quem é você?" #@param{type: "string"}

template = ChatPromptTemplate.from_messages([
    ("system", "Você é um redator profissional."),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
res

"""## Melhorando a exibição do resultado"""

def show_res(res):
    from IPython.display import Markdown
    display(Markdown(res))

show_res(res)

"""### Juntando em uma função"""

def llm_generate(llm, prompt):
    template = ChatPromptTemplate.from_messages([
        ("system", "Você é um redator profissional."),
        ("human", "{prompt}"),
    ])

    chain = template | llm | StrOutputParser()

    res = chain.invoke({"prompt": prompt})
    show_res(res)

prompt = "Escreva 5 dicas de saúde" #@param{type: "string"}

llm_generate(llm, prompt)

prompt = "Escreva 5 dicas de saúde mental" #@param{type: "string"}

llm_generate(llm, prompt)

prompt = "Escreva 5 dicas para quem quer ser arquiteto" #@param{type: "string"}

llm_generate(llm, prompt)

prompt = "Escreva quais são as 5 ferramentas principais de um arquiteto" #@param{type: "string"}

llm_generate(llm, prompt)

prompt = "como reconhecer e diferenciar depressao de bipolaridade" #@param{type: "string"}

llm_generate(llm, prompt)

"""# Construindo o promp de aplicação

#### Estrutura de um prompt

Existem várias técnicas de engenharia de prompt, onde muitdas delas se baseiam em princípions parecidos. Uma abordagem simples para construiz um prompt mais completo é adicionar a ele alguns 'Blocos' (componentes), que no caso seriam:
*   Papel (Role) - "quem" ele deve interpretar (mais sobre isso abaixo)
*   Tarefa (Task) - tarefa que deve realizar
*   Entrada (Input) - informação que pode ser usada como contexto para gerar uma resposta (por exemplo o faturamento mensal de uma empresa, ou um dado específico sobre algo ou alguém)
*   Saída (Output) - qualquer que seja o resultado. Podemos especificar também regras, como tamanho do resultado (medido em quantidade de palavras ou parágrafos por exemplo)
*   Restrições (Constraints) - o que queremos evitar na resposta. Por exemplo: "evite jargões ou linguagem muito técnica". "Não inclua sua análise ou opinião".

### Interpretação de papéis - Role Prompting
"""

prompt = "Fale sobre chocolate em 1 parágrafo"

template = ChatPromptTemplate.from_messages([
    ("system", "Você é um historiador profissional."),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
show_res(res)

template = ChatPromptTemplate.from_messages([
    ("system", "Você é um especialista em marketing digital."),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
show_res(res)

template = ChatPromptTemplate.from_messages([
    ("system", "Você é um especialista em marketing digital com foco em SEO e escrita persuasiva."),
    ("human", "{prompt}")
])

chain = template | llm | StrOutputParser()

res = chain.invoke({"prompt": prompt})
show_res(res)

"""### Usando exemplos - One-Shot e Few-Shot Prompting
*   Zero-Shot -- O modelo responde sem exemplos, confiando apenas no treinamento.
*   One-Shot -- Um exemplo é fornecido para orientar a resposta.
*   Few-Shot -- Vários exemplos ajudam o modelo a reconhecer padrões e melhorar a precisão.
"""

assunto = 'chocolate'

one_shot = f"""
Exemplo:
Título: Você sabia que beber mais água pode melhorar sua concentração?
Texto: A desidratação leve já é suficiente para reduzir seu foco e energia no dia a dia. Mantenha uma garrafinha por perto e lembre-se de se hidratar ao longo do dia.
Hashtags: #hidratação #foconasaude

Agora gere um novo texto que fale sobre {assunto}
"""

#print(one_shot)

res = chain.invoke({"prompt": one_shot})
show_res(res)

"""O few-shot prompting, ou prompt com exemplos, demonstra à IA a estrutura, estilo e abordagem desejados para a resposta, como a inclusão de hashtags ou a formulação de títulos como perguntas, tornando o processo de instrução mais intuitivo e eficiente do que apenas fornecer instruções textuais. Embora exemplos possam ser combinados com texto para maior precisão, o few-shot prompting com múltiplos exemplos, como o que veremos a seguir, ajuda a IA a generalizar melhor o padrão esperado."""

few_shot = f"""
Exemplo 1:
Título: Você sabia que beber mais água pode melhorar sua concentração?
Texto: A desidratação leve já é suficiente para reduzir seu foco e energia no dia a dia. Mantenha uma garrafinha por perto e lembre-se de se hidratar ao longo do dia.
Hashtags: #hidratação #foconasaude

Exemplo 2:
Título: Comer carboidratos à noite engorda: Mito ou verdade?
Texto: Esse é um mito comum. O que realmente importa é o total calórico do dia e a qualidade dos alimentos. Com orientação certa, dá sim para comer bem à noite sem culpa!
Hashtags: #nutricaosemmitos #equilibrioalimentar

Agora gere um novo texto que fale sobre {assunto}
"""

res = chain.invoke({"prompt": few_shot})
show_res(res)

"""#### Guiando o resultado com uma estrutura - Structured Prompting"""

form = create_form()
display(form)

prompt = f"""
Crie um post para {platform.value} com a seguinte estrutura:
1. Comece com uma pergunta provocativa.
2. Apresente um benefício claro relacionado ao tema.
3. Finalize com uma chamada para ação (CTA) encorajando o leitor a buscar mais informações.

Tema: {topic.value}
Público-alvo: {audience.value}
Tom: {tone.value}
"""

print(prompt)

res = chain.invoke({"prompt": prompt})
show_res(res)

"""### Construindo o prompt final dinamicamente

Este prompt final será construído a partir das variáveis do formulário, organizado em itens legíveis com - para fácil modificação e escalabilidade.

Cada linha fornecerá instruções claras (canal, tom, público...), e opções como hashtags ou CTAs serão incluídas condicionalmente usando expressões inline em Python, adaptando o prompt às escolhas do usuário.

Adicionaremos também a instrução para garantir que a saída seja limpa e pronta para uso.
"""

prompt = f"""
Escreva um texto com SEO otimizado sobre o tema '{topic.value}'.
Retorne em sua resposta apenas o texto final.
- Onde será publicado: {platform.value}.
- Tom: {tone.value}.
- Público-alvo: {audience.value}.
- Comprimento: {length.value}.
- {"Inclua uma chamada para ação clara." if cta.value else "Não inclua chamada para ação"}
- {"Retorne ao final do texto hashtags relevantes." if hashtags.value else "Não inclua hashtags."}
{"- Palavras-chave que devem estar presentes nesse texto (para SEO): " + keywords.value if keywords.value else ""}
"""
print(prompt)

res = chain.invoke({"prompt": prompt})
show_res(res)

"""## Concluíndo a aplicação"""

def llm_generate(llm, prompt):
  template = ChatPromptTemplate.from_messages([
      ("system", "Você é um especialista em marketing digital com foco em SEO e escrita persuasiva."),
      ("human", "{prompt}"),
  ])

  chain = template | llm | StrOutputParser()

  res = chain.invoke({"prompt": prompt})
  return res

def generate_result(b):
  with output:
    output.clear_output()
    prompt = f"""
    Escreva um texto com SEO otimizado sobre o tema '{topic.value}'.
    Retorne em sua resposta apenas o texto final e não inclua ela dentro de aspas.
    - Onde será publicado: {platform.value}.
    - Tom: {tone.value}.
    - Público-alvo: {audience.value}.
    - Comprimento: {length.value}.
    - {"Inclua uma chamada para ação clara." if cta.value else "Não inclua chamada para ação"}
    - {"Retorne ao final do texto hashtags relevantes." if hashtags.value else "Não inclua hashtags."}
    {"- Palavras-chave que devem estar presentes nesse texto (para SEO): " + keywords.value if keywords.value else ""}
    """
    try:
      res = llm_generate(llm, prompt)
      show_res(res)
    except Exception as e:
      print(f"Erro: {e}")

output = widgets.Output()
generate_button = widgets.Button(description = "Gerar conteúdo")
generate_button.on_click(generate_result)
form = create_form()

display(form)

"""## Escalando para outras áreas e adicionando mais

---

campos

Para aumentar a flexibilidade na definição das opções dos campos Dropdown, em vez de fixá-las no código, utilizaremos os formulários do Colab com a anotação `@param {type:"string"}`. Isso permite que o usuário insira uma lista de valores separados por vírgula diretamente em um campo ao lado da célula de código, que é então convertida em uma lista Python e usada dinamicamente no parâmetro options do widget.

Dessa forma, o formulário se torna totalmente configurável, permitindo fácil adição ou modificação das opções dos dropdowns, como as do campo "comprimento", sem alterar o código principal.

"""

opt_length = "Curto, Médio, Longo, 1 parágrafo, 1 página" # @param {type:"string"}
print(opt_length)

options_length = [x.strip() for x in opt_length.split(",")]

length = widgets.Dropdown(
    options = options_length,
    description="Tamanho",
    layout=widgets.Layout(width=w_dropdown)
)

form = create_form()
output.clear_output()
display(form)

"""## Construção de interface com Streamlit

#### 1. Instalação do Streamlit
"""

!pip install -q streamlit
!npm install -q localtunnel
!pip install -q python-dotenv

"""### 2. Criação do arquivo da aplicação

Crie um arquivo chamado `app.py` (ou outro nome que preferir) com o conteúdo do seu código adaptado para Streamlit.

Antes de colocarmos o código nesse arquivo, vamos criar o arquivo .env, para carregar as variáveis de ambiente. Aqui basta colocarmos a key do Groq, a mesma que usamos anteriormente. Deixe nesse formato: `GROQ_API_KEY=CHAVE_AQUI`

* Obs: o comando `%%writefile` no início desse bloco de código permite que a célula do notebook seja salva como um arquivo externo, com o nome especificado. Ou seja, estamos criando um arquivo com esse nome e o conteúdo será tudo a partir da segunda linha do bloco abaixo

"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile .env
# GROQ_API_KEY=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

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from langchain_groq import ChatGroq
# from langchain_core.prompts import ChatPromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from dotenv import load_dotenv
# load_dotenv()
# 
# ## conexão com a LLM
# id_model = "llama3-70b-8192"
# llm = ChatGroq(
#     model=id_model,
#     temperature=0.7,
#     max_tokens=None,
#     timeout=None,
#     max_retries=2,
# )
# 
# ## função de geração
# def llm_generate(llm, prompt):
#   template = ChatPromptTemplate.from_messages([
#       ("system", "Você é um especialista em marketing digital com foco em SEO e escrita persuasiva."),
#       ("human", "{prompt}"),
#   ])
# 
#   chain = template | llm | StrOutputParser()
# 
#   res = chain.invoke({"prompt": prompt})
#   return res
# 
# st.set_page_config(page_title = "Gerador de conteúdo 🤖", page_icon="🤖")
# st.title("Gerador de conteúdo")
# 
# # Campos do formulário
# topic = st.text_input("Tema:", placeholder="Ex: saúde mental, alimentação saudável, prevenção, etc.")
# platform = st.selectbox("Plataforma:", ['Instagram', 'Facebook', 'LinkedIn', 'Blog', 'E-mail'])
# tone = st.selectbox("Tom:", ['Normal', 'Informativo', 'Inspirador', 'Urgente', 'Informal'])
# length = st.selectbox("Tamanho:", ['Curto', 'Médio', 'Longo'])
# audience = st.selectbox("Público-alvo:", ['Geral', 'Jovens adultos', 'Famílias', 'Idosos', 'Adolescentes'])
# cta = st.checkbox("Incluir CTA")
# hashtags = st.checkbox("Retornar Hashtags")
# keywords = st.text_area("Palavras-chave (SEO):", placeholder="Ex: bem-estar, medicina preventiva...")
# 
# if st.button("Gerar conteúdo"):
#   prompt = f"""
#   Escreva um texto com SEO otimizado sobre o tema '{topic}'.
#   Retorne em sua resposta apenas o texto final e não inclua ela dentro de aspas.
#   - Onde será publicado: {platform}.
#   - Tom: {tone}.
#   - Público-alvo: {audience}.
#   - Comprimento: {length}.
#   - {"Inclua uma chamada para ação clara." if cta else "Não inclua chamada para ação"}
#   - {"Retorne ao final do texto hashtags relevantes." if hashtags else "Não inclua hashtags."}
#   {"- Palavras-chave que devem estar presentes nesse texto (para SEO): " + keywords if keywords else ""}
#   """
#   try:
#       res = llm_generate(llm, prompt)
#       st.markdown(res)
#   except Exception as e:
#       st.error(f"Erro: {e}")

"""### 3. Execução do Streamlit

Tendo nosso script pronto, basta executar o comando abaixo para rodar a nossa aplicação pelo streamlit.
Isso fará com que a aplicação do Streamlit seja executada em segundo plano.
"""

!streamlit run app.py &>/content/logs.txt &

!wget -q -O - ipv4.icanhazip.com
!npx localtunnel --port 8501

"""---

## Rodando a LLM localmente

Se for um modelo open source nós podemos fazer o download e rodar localmente em um provedor cloud (como nesse caso o colab) ou em nosso próprio computador.

### -> Para executar no Colab

**Importante:** Antes de realizar os próximos passos, mude o ambiente de execução no Colab para usar GPU, que será necessário já que todo o processamento será feito direto localmente no ambiente de execução do Colab. Para isso, selecione 'Ambiente de execução > Alterar o tipo de ambiente de execução' e na opção 'Acelerador de hardware' selecione 'GPU'.

Além das bibliotecas do langchain que instalamos, vamos precisar também da biblioteca `langchain-huggingface`, `transformers` e `bitsandbytes`
"""

!pip install -q langchain langchain-community langchain-huggingface transformers

!pip install bitsandbytes-cuda110 bitsandbytes

from langchain_huggingface import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig
import torch

"""**Quantização**

A execução de LLMs pode ser desafiadora devido aos recursos limitados, especialmente na versão gratuita do Google Colab. Para contornar essa limitação, além de escolher modelos com menos parâmetros podemos usar técnicas de quantização, como o `BitsAndBytesConfig` da biblioteca `transformers`, que permitem carregar e executar modelos massivos de forma eficiente sem comprometer significativamente o desempenho.
* Essas técnicas reduzem os custos de memória e computação ao representar pesos e ativações com tipos de dados de menor precisão, como inteiros de 8 bits (int8) ou até 4 bits, tornando viável o uso de modelos grandes mesmo em hardware limitado.
* Alternativas ao BitsAndBytesConfig: AutoGPTQ, AutoAWQ, etc.
* Para quem prefere evitar configurações complexas de otimização e manter a máxima qualidade, considere o uso via API.
* Mais detalhes sobre quantização: https://huggingface.co/blog/4bit-transformers-bitsandbytes
"""

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

"""**Download do modelo**

Agora faremos o download e a configuração de um modelo do HuggingFace usando o método `AutoModelForCausalLM.from_pretrained`. Este processo pode levar alguns minutos, pois o modelo tem alguns GB - mas no geral o download no Colab deve ser relativamente rápido.

> Para ver todos os modelos disponíveis no Hugging Face, acesse: https://huggingface.co/models?pipeline_tag=text-generation

Escolhemos o Phi 3 (microsoft/Phi-3-mini-4k-instruct), um modelo menor mas que demonstrou ser muito interessante e com ótimo custo benefício
 - https://huggingface.co/microsoft/Phi-3-mini-4k-instruct


"""

